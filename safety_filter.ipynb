{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.ngc.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "Collecting google-cloud-aiplatform\n",
      "  Downloading google_cloud_aiplatform-1.40.0-py2.py3-none-any.whl (3.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.4 MB 7.9 MB/s eta 0:00:01\n",
      "\u001b[?25h/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.ngc.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0\n",
      "  Downloading google_api_core-2.16.2-py3-none-any.whl (135 kB)\n",
      "\u001b[K     |████████████████████████████████| 135 kB 11.5 MB/s eta 0:00:01\n",
      "\u001b[?25h/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.ngc.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "Collecting google-cloud-storage<3.0.0dev,>=1.32.0\n",
      "  Downloading google_cloud_storage-2.14.0-py2.py3-none-any.whl (121 kB)\n",
      "\u001b[K     |████████████████████████████████| 121 kB 10.0 MB/s eta 0:00:01\n",
      "\u001b[?25h/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.ngc.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
      "  Downloading google_cloud_resource_manager-1.12.1-py2.py3-none-any.whl (333 kB)\n",
      "\u001b[K     |████████████████████████████████| 333 kB 10.3 MB/s eta 0:00:01\n",
      "\u001b[?25h/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.ngc.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.0\n",
      "  Downloading proto_plus-1.23.0-py3-none-any.whl (48 kB)\n",
      "\u001b[K     |████████████████████████████████| 48 kB 11.5 MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: packaging>=14.3 in ./.venv/lib/python3.8/site-packages (from google-cloud-aiplatform) (23.2)\n",
      "/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.ngc.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "Collecting google-cloud-bigquery<4.0.0dev,>=1.15.0\n",
      "  Downloading google_cloud_bigquery-3.17.2-py2.py3-none-any.whl (230 kB)\n",
      "\u001b[K     |████████████████████████████████| 230 kB 10.3 MB/s eta 0:00:01\n",
      "\u001b[?25h/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.ngc.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5\n",
      "  Downloading protobuf-4.25.2-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[K     |████████████████████████████████| 294 kB 11.6 MB/s eta 0:00:01\n",
      "\u001b[?25h/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.ngc.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "Collecting shapely<3.0.0dev\n",
      "  Downloading shapely-2.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.5 MB 9.4 MB/s eta 0:00:01\n",
      "\u001b[?25h/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.ngc.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.62.0-py2.py3-none-any.whl (228 kB)\n",
      "\u001b[K     |████████████████████████████████| 228 kB 10.6 MB/s eta 0:00:01\n",
      "\u001b[?25h/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.ngc.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "Collecting requests<3.0.0.dev0,>=2.18.0\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 13.8 MB/s eta 0:00:01\n",
      "\u001b[?25h/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.ngc.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "Collecting google-auth<3.0.dev0,>=2.14.1\n",
      "  Downloading google_auth-2.27.0-py2.py3-none-any.whl (186 kB)\n",
      "\u001b[K     |████████████████████████████████| 186 kB 10.0 MB/s eta 0:00:01\n",
      "\u001b[?25h/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.ngc.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "Collecting grpcio<2.0dev,>=1.33.2; extra == \"grpc\"\n",
      "  Downloading grpcio-1.60.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.4 MB 9.3 MB/s eta 0:00:01\n",
      "\u001b[?25h/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.ngc.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2; extra == \"grpc\"\n",
      "  Downloading grpcio_status-1.60.1-py3-none-any.whl (14 kB)\n",
      "/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.ngc.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.ngc.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "Collecting google-cloud-core<3.0dev,>=2.3.0\n",
      "  Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl (29 kB)\n",
      "/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.ngc.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "Collecting google-resumable-media>=2.6.0\n",
      "  Downloading google_resumable_media-2.7.0-py2.py3-none-any.whl (80 kB)\n",
      "\u001b[K     |████████████████████████████████| 80 kB 11.9 MB/s eta 0:00:01\n",
      "\u001b[?25h/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.ngc.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4\n",
      "  Downloading grpc_google_iam_v1-0.13.0-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0dev,>=2.7.2 in ./.venv/lib/python3.8/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
      "/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.ngc.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "Collecting numpy>=1.14\n",
      "  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.3 MB 10.0 MB/s eta 0:00:01\n",
      "\u001b[?25h/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.ngc.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 5.3 MB/s  eta 0:00:01\n",
      "\u001b[?25h/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.ngc.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.2.0-py3-none-any.whl (120 kB)\n",
      "\u001b[K     |████████████████████████████████| 120 kB 11.5 MB/s eta 0:00:01\n",
      "\u001b[?25h/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.ngc.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "\u001b[K     |████████████████████████████████| 141 kB 15.6 MB/s eta 0:00:01\n",
      "\u001b[?25h/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.ngc.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "\u001b[K     |████████████████████████████████| 163 kB 9.8 MB/s eta 0:00:01\n",
      "\u001b[?25h/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.ngc.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 10.0 MB/s eta 0:00:01\n",
      "\u001b[?25h/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.ngc.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.ngc.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in ./.venv/lib/python3.8/site-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.16.0)\n",
      "/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.ngc.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[K     |████████████████████████████████| 84 kB 8.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: protobuf, googleapis-common-protos, idna, urllib3, charset-normalizer, certifi, requests, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, grpcio, grpcio-status, google-api-core, google-crc32c, google-cloud-core, google-resumable-media, google-cloud-storage, grpc-google-iam-v1, proto-plus, google-cloud-resource-manager, google-cloud-bigquery, numpy, shapely, google-cloud-aiplatform\n",
      "Successfully installed cachetools-5.3.2 certifi-2024.2.2 charset-normalizer-3.3.2 google-api-core-2.16.2 google-auth-2.27.0 google-cloud-aiplatform-1.40.0 google-cloud-bigquery-3.17.2 google-cloud-core-2.4.1 google-cloud-resource-manager-1.12.1 google-cloud-storage-2.14.0 google-crc32c-1.5.0 google-resumable-media-2.7.0 googleapis-common-protos-1.62.0 grpc-google-iam-v1-0.13.0 grpcio-1.60.1 grpcio-status-1.60.1 idna-3.6 numpy-1.24.4 proto-plus-1.23.0 protobuf-4.25.2 pyasn1-0.5.1 pyasn1-modules-0.3.0 requests-2.31.0 rsa-4.9 shapely-2.0.2 urllib3-2.2.0\n"
     ]
    }
   ],
   "source": [
    "! pip3 install --upgrade google-cloud-aiplatform\n",
    "#! pip3 install google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.preview.generative_models import (\n",
    "    GenerationConfig,\n",
    "    GenerativeModel,\n",
    "    Image,\n",
    "    Part,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "palm_model = GenerativeModel(\"text-bison@001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhausted",
     "evalue": "429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: text-bison. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_MultiThreadedRendezvous\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[0;32m~/devel/palm_test2/.venv/lib/python3.8/site-packages/google/api_core/grpc_helpers.py:173\u001b[0m, in \u001b[0;36m_wrap_stream_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     prefetch_first \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callable_, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_prefetch_first_result_\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_StreamingResponseIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefetch_first_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefetch_first\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/devel/palm_test2/.venv/lib/python3.8/site-packages/google/api_core/grpc_helpers.py:95\u001b[0m, in \u001b[0;36m_StreamingResponseIterator.__init__\u001b[0;34m(self, wrapped, prefetch_first_result)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prefetch_first_result:\n\u001b[0;32m---> 95\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stored_first_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;66;03m# It is possible the wrapped method isn't an iterable (a grpc.Call\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;66;03m# for instance). If this happens don't store the first result.\u001b[39;00m\n",
      "File \u001b[0;32m~/devel/palm_test2/.venv/lib/python3.8/site-packages/grpc/_channel.py:540\u001b[0m, in \u001b[0;36m_Rendezvous.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/devel/palm_test2/.venv/lib/python3.8/site-packages/grpc/_channel.py:966\u001b[0m, in \u001b[0;36m_MultiThreadedRendezvous._next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 966\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31m_MultiThreadedRendezvous\u001b[0m: <_MultiThreadedRendezvous of RPC that terminated with:\n\tstatus = StatusCode.RESOURCE_EXHAUSTED\n\tdetails = \"Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: text-bison. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:172.217.25.10:443 {grpc_message:\"Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: text-bison. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.\", grpc_status:8, created_time:\"2024-02-08T09:58:35.365075+09:00\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is a large language model?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mpalm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[0;32m~/devel/palm_test2/.venv/lib/python3.8/site-packages/vertexai/generative_models/_generative_models.py:353\u001b[0m, in \u001b[0;36m_GenerativeModel.generate_content\u001b[0;34m(self, contents, generation_config, safety_settings, tools, stream)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_content_streaming(\n\u001b[1;32m    347\u001b[0m         contents\u001b[38;5;241m=\u001b[39mcontents,\n\u001b[1;32m    348\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[1;32m    349\u001b[0m         safety_settings\u001b[38;5;241m=\u001b[39msafety_settings,\n\u001b[1;32m    350\u001b[0m         tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m    351\u001b[0m     )\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafety_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafety_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/devel/palm_test2/.venv/lib/python3.8/site-packages/vertexai/generative_models/_generative_models.py:437\u001b[0m, in \u001b[0;36m_GenerativeModel._generate_content\u001b[0;34m(self, contents, generation_config, safety_settings, tools)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# generate_content is not available\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m# gapic_response = self._prediction_client.generate_content(request=request)\u001b[39;00m\n\u001b[1;32m    436\u001b[0m gapic_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 437\u001b[0m stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prediction_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_generate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gapic_chunk \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gapic_response:\n",
      "File \u001b[0;32m~/devel/palm_test2/.venv/lib/python3.8/site-packages/google/cloud/aiplatform_v1beta1/services/prediction_service/client.py:1634\u001b[0m, in \u001b[0;36mPredictionServiceClient.stream_generate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1629\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(metadata) \u001b[38;5;241m+\u001b[39m (\n\u001b[1;32m   1630\u001b[0m     gapic_v1\u001b[38;5;241m.\u001b[39mrouting_header\u001b[38;5;241m.\u001b[39mto_grpc_metadata(((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmodel),)),\n\u001b[1;32m   1631\u001b[0m )\n\u001b[1;32m   1633\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m-> 1634\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1639\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1641\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/devel/palm_test2/.venv/lib/python3.8/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/devel/palm_test2/.venv/lib/python3.8/site-packages/google/api_core/grpc_helpers.py:177\u001b[0m, in \u001b[0;36m_wrap_stream_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StreamingResponseIterator(\n\u001b[1;32m    174\u001b[0m         result, prefetch_first_result\u001b[38;5;241m=\u001b[39mprefetch_first\n\u001b[1;32m    175\u001b[0m     )\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mResourceExhausted\u001b[0m: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: text-bison. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai."
     ]
    }
   ],
   "source": [
    "prompt = \"What is a large language model?\"\n",
    "\n",
    "response = palm_model.generate_content(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm.... Try it with changing the api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.language_models import (\n",
    "    TextGenerationModel,\n",
    "    TextEmbeddingModel,\n",
    "    ChatModel,\n",
    "    InputOutputTextPair,\n",
    "    CodeGenerationModel,\n",
    "    CodeChatModel,\n",
    ")\n",
    "\n",
    "text_model_old_api = TextGenerationModel.from_pretrained(\"text-bison@001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A large language model (LLM) is a type of artificial intelligence (AI) model that can understand and generate human language. LLMs are trained on massive datasets of text and code, and they can learn to perform a wide variety of tasks, such as translating languages, writing different kinds of creative content, and answering your questions in an informative way.\n",
      "\n",
      "LLMs are still under development, but they have the potential to revolutionize many industries. For example, LLMs could be used to create more accurate and personalized customer service experiences, to help doctors diagnose and treat diseases, and to even write entire books and movies.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is a large language model?\"\n",
    "\n",
    "response = text_model_old_api.predict(prompt=prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, new api - GenerativeModel - is for Gemini not for old text-bison model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidates {\n",
      "  content {\n",
      "    role: \"model\"\n",
      "    parts {\n",
      "      text: \"A large language model (LLM) is a type of artificial intelligence (AI) that can understand and generate human language. LLMs are trained on massive datasets of text, which allows them to learn the statistical patterns of language and predict the next word in a sequence with high accuracy. This enables them to perform a wide range of language-related tasks, such as:\\n\\n* **Text generation:** LLMs can generate coherent and grammatically correct text on a variety of topics, including news articles, stories, poems, and code.\\n* **Translation:** LLMs can translate text between different languages, even if they have not been explicitly trained to do so.\\n* **Summarization:** LLMs can summarize long documents or articles into shorter, more concise versions.\\n* **Question answering:** LLMs can answer questions about the world, even if the answer is not explicitly stated in the text they were trained on.\\n* **Sentiment analysis:** LLMs can determine the sentiment of a piece of text, such as whether it is positive, negative, or neutral.\\n* **Named entity recognition:** LLMs can identify and extract named entities from text, such as people, places, and organizations.\\n\\nLLMs have been used to create a variety of innovative applications, such as:\\n\\n* **Chatbots:** LLMs can be used to create chatbots that can converse with humans in a natural way.\\n* **Virtual assistants:** LLMs can be used to create virtual assistants that can help users with a variety of tasks, such as scheduling appointments, sending emails, and finding information online.\\n* **Language translation services:** LLMs can be used to create language translation services that can translate text between different languages quickly and accurately.\\n* **Content generation tools:** LLMs can be used to create content generation tools that can help writers with tasks such as brainstorming ideas, writing headlines, and generating product descriptions.\\n\\nLLMs are still under development, but they have the potential to revolutionize the way we interact with computers. They offer a powerful new way to generate and understand human language, and they are likely to be used in a wide range of applications in the years to come.\"\n",
      "    }\n",
      "  }\n",
      "  finish_reason: STOP\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HARASSMENT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HATE_SPEECH\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "}\n",
      "usage_metadata {\n",
      "  prompt_token_count: 7\n",
      "  candidates_token_count: 441\n",
      "  total_token_count: 448\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gemini_model = GenerativeModel(\"gemini-pro\")\n",
    "\n",
    "prompt = \"What is a large language model?\"\n",
    "\n",
    "response = gemini_model.generate_content(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with safety filter\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    max_output_tokens=2048,\n",
    ")\n",
    "\n",
    "safety_config = {\n",
    "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "}\n",
    "\n",
    "response = gemini_model.generate_content(prompt, generation_config=generation_config, \n",
    "    safety_settings=safety_config, stream=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidates {\n",
      "  content {\n",
      "    role: \"model\"\n",
      "    parts {\n",
      "      text: \"A large language model (LLM) is a type of artificial intelligence (AI) that can understand and generate human language. LLMs are trained on massive datasets of text, which allows them to learn the patterns and structures of language. This enables them to perform a wide range of language-related tasks, such as:\\n\\n* **Text generation:** LLMs can generate text that is both coherent and grammatically correct. This can be used for a variety of applications, such as writing articles, stories, and poems.\\n* **Language translation:** LLMs can translate text from one language to another. This can be used for a variety of applications, such as providing real-time translation for websites and documents.\\n* **Question answering:** LLMs can answer questions about a wide range of topics. This can be used for a variety of applications, such as providing customer support and answering FAQs.\\n* **Summarization:** LLMs can summarize text into a shorter, more concise version. This can be used for a variety of applications, such as providing summaries of news articles and research papers.\\n\\nLLMs are still under development, but they have the potential to revolutionize the way we interact with computers. They could make it possible for us to communicate with computers in a more natural way, and they could also help us to automate a wide range of tasks that currently require human input.\\n\\nHere are some of the key characteristics of LLMs:\\n\\n* **Size:** LLMs are typically very large, with billions or even trillions of parameters. This allows them to learn a vast amount of information about language.\\n* **Data:** LLMs are trained on massive datasets of text. This data can come from a variety of sources, such as books, articles, and websites.\\n* **Architecture:** LLMs typically use a transformer-based architecture. Transformers are a type of neural network that is well-suited for processing sequential data, such as text.\\n* **Training:** LLMs are trained using a variety of techniques, such as supervised learning and unsupervised learning. Supervised learning involves training the model on a dataset of labeled data, while unsupervised learning involves training the model on a dataset of unlabeled data.\\n\\nLLMs are a powerful tool that has the potential to revolutionize the way we interact with computers. They are still under development, but they are already showing great promise for a wide range of applications.\"\n",
      "    }\n",
      "  }\n",
      "  finish_reason: STOP\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HARASSMENT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HATE_SPEECH\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "}\n",
      "usage_metadata {\n",
      "  prompt_token_count: 7\n",
      "  candidates_token_count: 489\n",
      "  total_token_count: 496\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try another aggrevssive prompt\n",
    "\n",
    "prompt = \"what's the history of the idiot?\"\n",
    "\n",
    "response = gemini_model.generate_content(prompt, generation_config=generation_config, \n",
    "    safety_settings=safety_config, stream=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidates {\n",
      "  content {\n",
      "    role: \"model\"\n",
      "    parts {\n",
      "      text: \"The term \\\"idiot\\\" has a long and complex history, dating back to ancient Greece. In ancient Greek, the word \\\"idiotēs\\\" meant \\\"a private person,\\\" as opposed to someone who held public office. The term was also used to refer to someone who was ignorant or unskilled.\\n\\nIn the Middle Ages, the term \\\"idiot\\\" began to be used in a more negative sense. It was often used to refer to someone who was mentally disabled or foolish. This usage of the term continued into the early modern period.\\n\\nIn the 19th century, the term \\\"idiot\\\" began to be used in a more clinical sense. It was used to refer to someone who had a severe intellectual disability. This usage of the term is still used today.\\n\\nThe history of the term \\\"idiot\\\" is a reflection of the changing attitudes towards people with disabilities. In ancient Greece, people with disabilities were often seen as being less than human. In the Middle Ages, they were often seen as being possessed by demons. In the early modern period, they were often seen as being lazy or immoral.\\n\\nToday, people with disabilities are seen as being just as valuable as anyone else. They are entitled to the same rights and opportunities as everyone else. The term \\\"idiot\\\" is no longer used in a clinical sense, and it is considered to be offensive.\\n\\nHere is a timeline of some of the key events in the history of the term \\\"idiot\\\":\\n\\n* **Ancient Greece:** The term \\\"idiotēs\\\" is used to refer to a private person, as opposed to someone who holds public office.\\n* **Middle Ages:** The term \\\"idiot\\\" begins to be used in a more negative sense, to refer to someone who is mentally disabled or foolish.\\n* **Early modern period:** The term \\\"idiot\\\" continues to be used in a negative sense, and is often used to refer to someone who is lazy or immoral.\\n* **19th century:** The term \\\"idiot\\\" begins to be used in a more clinical sense, to refer to someone who has a severe intellectual disability.\\n* **20th century:** The term \\\"idiot\\\" is no longer used in a clinical sense, and is considered to be offensive.\\n\\nToday, the term \\\"idiot\\\" is rarely used, and is considered to be offensive. It is important to remember that people with disabilities are just as valuable as anyone else, and they are entitled to the same rights and opportunities as everyone else.\"\n",
      "    }\n",
      "  }\n",
      "  finish_reason: STOP\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HARASSMENT\n",
      "    probability: MEDIUM\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HATE_SPEECH\n",
      "    probability: LOW\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "}\n",
      "usage_metadata {\n",
      "  prompt_token_count: 9\n",
      "  candidates_token_count: 504\n",
      "  total_token_count: 513\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
